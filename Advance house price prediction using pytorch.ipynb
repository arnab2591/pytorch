{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('houseprice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be creating our dataframe with some selective features\n",
    "df = pd.read_csv('houseprice.csv',usecols=[\"SalePrice\",\"MSSubClass\",\"MSZoning\",\"LotFrontage\",\"LotArea\",\"Street\",\\\n",
    "                                          \"YearBuilt\",\"LotShape\",\"1stFlrSF\",\"2ndFlrSF\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1915</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  YearBuilt  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       2003   \n",
       "1          20       RL         80.0     9600   Pave      Reg       1976   \n",
       "2          60       RL         68.0    11250   Pave      IR1       2001   \n",
       "3          70       RL         60.0     9550   Pave      IR1       1915   \n",
       "4          60       RL         84.0    14260   Pave      IR1       2000   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0       856       854     208500  \n",
       "1      1262         0     181500  \n",
       "2       920       866     223500  \n",
       "3       961       756     140000  \n",
       "4      1145      1053     250000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      "MSSubClass     1201 non-null int64\n",
      "MSZoning       1201 non-null object\n",
      "LotFrontage    1201 non-null float64\n",
      "LotArea        1201 non-null int64\n",
      "Street         1201 non-null object\n",
      "LotShape       1201 non-null object\n",
      "YearBuilt      1201 non-null int64\n",
      "1stFlrSF       1201 non-null int64\n",
      "2ndFlrSF       1201 non-null int64\n",
      "SalePrice      1201 non-null int64\n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. categorical features - Embedding layers\n",
    "2. continuous Features-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name MSSubClass and unique values are 15\n",
      "Column name MSZoning and unique values are 5\n",
      "Column name LotFrontage and unique values are 110\n",
      "Column name LotArea and unique values are 869\n",
      "Column name Street and unique values are 2\n",
      "Column name LotShape and unique values are 4\n",
      "Column name YearBuilt and unique values are 112\n",
      "Column name 1stFlrSF and unique values are 678\n",
      "Column name 2ndFlrSF and unique values are 368\n",
      "Column name SalePrice and unique values are 597\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(\"Column name {} and unique values are {}\".format(i,len(df[i].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 8, 26, 20, 39, 50, 813019)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable of year count\n",
    "df['Total Years']=datetime.datetime.now().year-df['YearBuilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping variables of year built\n",
    "df.drop('YearBuilt',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  1stFlrSF  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       856   \n",
       "1          20       RL         80.0     9600   Pave      Reg      1262   \n",
       "2          60       RL         68.0    11250   Pave      IR1       920   \n",
       "3          70       RL         60.0     9550   Pave      IR1       961   \n",
       "4          60       RL         84.0    14260   Pave      IR1      1145   \n",
       "\n",
       "   2ndFlrSF  SalePrice  Total Years  \n",
       "0       854     208500           17  \n",
       "1         0     181500           44  \n",
       "2       866     223500           19  \n",
       "3       756     140000          105  \n",
       "4      1053     250000           20  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'LotShape', '1stFlrSF', '2ndFlrSF', 'SalePrice', 'Total Years'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Categorical features\n",
    "cat_features = [\"MSSubClass\",\"MSZoning\",\"Street\",\"LotShape\"]\n",
    "out_feature = \"SalePrice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60,  20,  70,  50, 190,  45,  90, 120,  30,  80, 160,  75, 180,\n",
       "        40,  85], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"MSSubClass\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, ..., 6, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoder = {}\n",
    "lbl_encoder[\"MSSubClass\"]=LabelEncoder()\n",
    "\n",
    "lbl_encoder[\"MSSubClass\"].fit_transform(df[\"MSSubClass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  1stFlrSF  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       856   \n",
       "1          20       RL         80.0     9600   Pave      Reg      1262   \n",
       "2          60       RL         68.0    11250   Pave      IR1       920   \n",
       "3          70       RL         60.0     9550   Pave      IR1       961   \n",
       "4          60       RL         84.0    14260   Pave      IR1      1145   \n",
       "\n",
       "   2ndFlrSF  SalePrice  Total Years  \n",
       "0       854     208500           17  \n",
       "1         0     181500           44  \n",
       "2       866     223500           19  \n",
       "3       756     140000          105  \n",
       "4      1053     250000           20  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSSubClass': LabelEncoder()}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoder = {}\n",
    "for feature in cat_features:\n",
    "    lbl_encoder[feature]=LabelEncoder()\n",
    "    df[feature]=lbl_encoder[feature].fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "      <td>566</td>\n",
       "      <td>143000</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1694</td>\n",
       "      <td>0</td>\n",
       "      <td>307000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1022</td>\n",
       "      <td>752</td>\n",
       "      <td>129900</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1077</td>\n",
       "      <td>0</td>\n",
       "      <td>118000</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1040</td>\n",
       "      <td>0</td>\n",
       "      <td>129500</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11924</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1182</td>\n",
       "      <td>1142</td>\n",
       "      <td>345000</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>91.0</td>\n",
       "      <td>10652</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1494</td>\n",
       "      <td>0</td>\n",
       "      <td>279500</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>132000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10791</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "      <td>90000</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>66.0</td>\n",
       "      <td>13695</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1114</td>\n",
       "      <td>0</td>\n",
       "      <td>159000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1339</td>\n",
       "      <td>0</td>\n",
       "      <td>139000</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>101.0</td>\n",
       "      <td>14215</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1158</td>\n",
       "      <td>1218</td>\n",
       "      <td>325300</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7449</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1108</td>\n",
       "      <td>0</td>\n",
       "      <td>139400</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9742</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1795</td>\n",
       "      <td>0</td>\n",
       "      <td>230000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>129900</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  1stFlrSF  \\\n",
       "0            5         3         65.0     8450       1         3       856   \n",
       "1            0         3         80.0     9600       1         3      1262   \n",
       "2            5         3         68.0    11250       1         0       920   \n",
       "3            6         3         60.0     9550       1         0       961   \n",
       "4            5         3         84.0    14260       1         0      1145   \n",
       "5            4         3         85.0    14115       1         0       796   \n",
       "6            0         3         75.0    10084       1         3      1694   \n",
       "8            4         4         51.0     6120       1         3      1022   \n",
       "9           14         3         50.0     7420       1         3      1077   \n",
       "10           0         3         70.0    11200       1         3      1040   \n",
       "11           5         3         85.0    11924       1         0      1182   \n",
       "13           0         3         91.0    10652       1         0      1494   \n",
       "15           3         4         51.0     6120       1         3       854   \n",
       "17          10         3         72.0    10791       1         3      1296   \n",
       "18           0         3         66.0    13695       1         3      1114   \n",
       "19           0         3         70.0     7560       1         3      1339   \n",
       "20           5         3        101.0    14215       1         0      1158   \n",
       "21           3         4         57.0     7449       1         3      1108   \n",
       "22           0         3         75.0     9742       1         3      1795   \n",
       "23          11         4         44.0     4224       1         3      1060   \n",
       "\n",
       "    2ndFlrSF  SalePrice  Total Years  \n",
       "0        854     208500           17  \n",
       "1          0     181500           44  \n",
       "2        866     223500           19  \n",
       "3        756     140000          105  \n",
       "4       1053     250000           20  \n",
       "5        566     143000           27  \n",
       "6          0     307000           16  \n",
       "8        752     129900           89  \n",
       "9          0     118000           81  \n",
       "10         0     129500           55  \n",
       "11      1142     345000           15  \n",
       "13         0     279500           14  \n",
       "15         0     132000           91  \n",
       "17         0      90000           53  \n",
       "18         0     159000           16  \n",
       "19         0     139000           62  \n",
       "20      1218     325300           15  \n",
       "21         0     139400           90  \n",
       "22         0     230000           18  \n",
       "23         0     129900           44  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical feature converting into \n",
    "#--label encoding\n",
    "#--convert all categorical variable to numpy -> tensors(torch) -> \n",
    "## Embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Stocking and Converting into Tensors\n",
    "cat_features= np.stack([df[\"MSSubClass\"],df[\"MSZoning\"],df[\"Street\"],df[\"LotShape\"]],axis=1)\n",
    "cat_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [5, 3, 1, 0],\n",
       "       ...,\n",
       "       [6, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [0, 3, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert numpy to Tensors\n",
    "# for categorical feature ,it will be always integer type\n",
    "import torch\n",
    "cat_features=torch.tensor(cat_features,dtype=torch.int64)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Size for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"Street\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 2, 4]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get all the dimension into categorical value\n",
    "cat_dims=[len(df[i].unique()) for i in [\"MSSubClass\",\"MSZoning\",\"Street\",\"LotShape\"]]\n",
    "cat_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (2, 1), (4, 2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Thumb rule for embedding layers .Output dimension should be set based on the input dimension (min(50,feature dimension/2))\n",
    "embedding_dim= [(x,min(50,(x+1)//2)) for x in cat_dims]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(2, 1)\n",
       "  (3): Embedding(4, 2)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "embed_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        [6, 3, 1, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_featuresz=cat_features[:4]\n",
    "cat_featuresz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',500)\n",
    "embedding_val=[]\n",
    "for i,e in enumerate(embed_representation):\n",
    "    embedding_val.append(e(cat_features[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0254,  0.0373, -1.0947,  ...,  0.7105,  0.0182,  1.8779],\n",
       "         [ 1.2927, -1.1653,  0.3781,  ..., -0.9210,  0.5849,  0.3617],\n",
       "         [-0.0254,  0.0373, -1.0947,  ...,  0.7105,  0.0182,  1.8779],\n",
       "         ...,\n",
       "         [ 1.2691, -1.3445, -1.1262,  ..., -0.8694,  0.6795, -1.2289],\n",
       "         [ 1.2927, -1.1653,  0.3781,  ..., -0.9210,  0.5849,  0.3617],\n",
       "         [ 1.2927, -1.1653,  0.3781,  ..., -0.9210,  0.5849,  0.3617]],\n",
       "        grad_fn=<EmbeddingBackward>),\n",
       " tensor([[ 0.3775, -0.9475, -0.6032],\n",
       "         [ 0.3775, -0.9475, -0.6032],\n",
       "         [ 0.3775, -0.9475, -0.6032],\n",
       "         ...,\n",
       "         [ 0.3775, -0.9475, -0.6032],\n",
       "         [ 0.3775, -0.9475, -0.6032],\n",
       "         [ 0.3775, -0.9475, -0.6032]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[-0.4264],\n",
       "         [-0.4264],\n",
       "         [-0.4264],\n",
       "         ...,\n",
       "         [-0.4264],\n",
       "         [-0.4264],\n",
       "         [-0.4264]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[ 0.4589,  0.3637],\n",
       "         [ 0.4589,  0.3637],\n",
       "         [ 0.6947, -0.6387],\n",
       "         ...,\n",
       "         [ 0.4589,  0.3637],\n",
       "         [ 0.4589,  0.3637],\n",
       "         [ 0.4589,  0.3637]], grad_fn=<EmbeddingBackward>)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0254,  0.0373, -1.0947,  ..., -0.4264,  0.4589,  0.3637],\n",
       "        [ 1.2927, -1.1653,  0.3781,  ..., -0.4264,  0.4589,  0.3637],\n",
       "        [-0.0254,  0.0373, -1.0947,  ..., -0.4264,  0.6947, -0.6387],\n",
       "        ...,\n",
       "        [ 1.2691, -1.3445, -1.1262,  ..., -0.4264,  0.4589,  0.3637],\n",
       "        [ 1.2927, -1.1653,  0.3781,  ..., -0.4264,  0.4589,  0.3637],\n",
       "        [ 1.2927, -1.1653,  0.3781,  ..., -0.4264,  0.4589,  0.3637]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.cat(embedding_val,axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement Dropout to prevent over fitting inside embedding layer\n",
    "dropout=nn.Dropout(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0254,  0.0373, -1.0947,  ..., -0.4264,  0.4589,  0.3637],\n",
       "        [ 1.2927, -1.1653,  0.3781,  ..., -0.4264,  0.4589,  0.3637],\n",
       "        [-0.0254,  0.0373, -1.0947,  ..., -0.4264,  0.6947, -0.6387],\n",
       "        ...,\n",
       "        [ 1.2691, -1.3445, -1.1262,  ..., -0.4264,  0.4589,  0.3637],\n",
       "        [ 1.2927, -1.1653,  0.3781,  ..., -0.4264,  0.4589,  0.3637],\n",
       "        [ 1.2927, -1.1653,  0.3781,  ..., -0.4264,  0.4589,  0.3637]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embed = dropout(z)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to deal with continuous variable\n",
    "- select all continuous variable\n",
    "- convert into numpy array and then convert into torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take all the continuos value\n",
    "cont_variable=[]\n",
    "for i in df.columns:\n",
    "    if i in [\"MSSubClass\",\"MSZoning\",\"Street\",\"LotShape\",\"SalePrice\"]:\n",
    "        pass\n",
    "    else:\n",
    "        cont_variable.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'Total Years']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,   856.,   854.,    17.],\n",
       "        [   80.,  9600.,  1262.,     0.,    44.],\n",
       "        [   68., 11250.,   920.,   866.,    19.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1188.,  1152.,    79.],\n",
       "        [   68.,  9717.,  1078.,     0.,    70.],\n",
       "        [   75.,  9937.,  1256.,     0.,    55.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking continuos variable to a tensor\n",
    "cont_values = np.stack([df[i].values for i in cont_variable],axis=1)\n",
    "cont_values = torch.tensor(cont_values,dtype=torch.float)\n",
    "cont_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1201, 5])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[208500.],\n",
       "        [181500.],\n",
       "        [223500.],\n",
       "        ...,\n",
       "        [266500.],\n",
       "        [142125.],\n",
       "        [147500.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Dependent Feature\n",
    "y = torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      "MSSubClass     1201 non-null int64\n",
      "MSZoning       1201 non-null int32\n",
      "LotFrontage    1201 non-null float64\n",
      "LotArea        1201 non-null int64\n",
      "Street         1201 non-null int32\n",
      "LotShape       1201 non-null int32\n",
      "1stFlrSF       1201 non-null int64\n",
      "2ndFlrSF       1201 non-null int64\n",
      "SalePrice      1201 non-null int64\n",
      "Total Years    1201 non-null int64\n",
      "dtypes: float64(1), int32(3), int64(6)\n",
      "memory usage: 89.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  1stFlrSF  \\\n",
       "0           5         3         65.0     8450       1         3       856   \n",
       "1           0         3         80.0     9600       1         3      1262   \n",
       "2           5         3         68.0    11250       1         0       920   \n",
       "3           6         3         60.0     9550       1         0       961   \n",
       "4           5         3         84.0    14260       1         0      1145   \n",
       "\n",
       "   2ndFlrSF  SalePrice  Total Years  \n",
       "0       854     208500           17  \n",
       "1         0     181500           44  \n",
       "2       866     223500           19  \n",
       "3       756     140000          105  \n",
       "4      1053     250000           20  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1201, 4]), torch.Size([1201, 5]), torch.Size([1201, 1]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features.shape,cont_values.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a Feed Forward neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    \n",
    "    # embedding_dim means embedding shapes\n",
    "    # n_cont means numeric continuos value\n",
    "    # out_sz means output value\n",
    "    # p means dropout ratio\n",
    "    def __init__(self,embedding_dim,n_cont,out_sz,layers,p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)# Batch normalization with respect to number of continuous variable\n",
    "        \n",
    "        layerlist=[]\n",
    "        n_emb = sum((out for inp,out in embedding_dim))# total number of dimension for embedding layer\n",
    "        n_in = n_emb + n_cont # number of inputs\n",
    "        #layers = [100,50]\n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i))# Suppose there are 100 neuron in the first hidden layer\n",
    "            layerlist.append(nn.ReLU(inplace=True))# Then relu activation function will be applied \n",
    "            layerlist.append(nn.BatchNorm1d(i))# then the batch normalization function\n",
    "            layerlist.append(nn.Dropout(p))# Then the drop out layer for the first hidden layer After the 1st iteration there\n",
    "            n_in = i # will be 2nd \n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))# finally it will connect the hidden layers to output layers,Here our\n",
    "        # output layer will be 1 because it is a regression problem ,has to predict the house price\n",
    "        \n",
    "        self.layers= nn.Sequential(*layerlist)# now we will add all the layers inside the sequential model\n",
    "        \n",
    "    def forward(self,x_cat,x_cont):\n",
    "        #x_cat is a categorical features\n",
    "        #x_cont is a continuous features\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i])) # Creating embedding layers for categorical variables\n",
    "        x = torch.cat(embeddings,1) # concating all the embedding layers\n",
    "        x = self.emb_drop(x) # implementing dropout layers\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x,x_cont],1)# concatinating continuous and categorical variable\n",
    "        x = self.layers(x)# giving inputs to our layers\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "#layers = [100,50]\n",
    "# output_layer = 1\n",
    "model = FeedForwardNN(embedding_dim,len(cont_variable),1,[100,50],p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss() ### later convert into RMSE\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,   856.,   854.,    17.],\n",
       "        [   80.,  9600.,  1262.,     0.,    44.],\n",
       "        [   68., 11250.,   920.,   866.,    19.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1188.,  1152.,    79.],\n",
       "        [   68.,  9717.,  1078.,     0.,    70.],\n",
       "        [   75.,  9937.,  1256.,     0.,    55.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1201, 5])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "torch.Size([1020, 4])\n",
      "torch.Size([180, 4])\n",
      "torch.Size([1020, 5])\n",
      "torch.Size([180, 5])\n",
      "torch.Size([1020, 1])\n",
      "torch.Size([180, 1])\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into train and test dataset\n",
    "batch_size=1200\n",
    "test_size=int(batch_size*0.15) # test data\n",
    "print(test_size)\n",
    "train_categorical = cat_features[:batch_size-test_size]\n",
    "print(train_categorical.shape)\n",
    "test_categorical = cat_features[batch_size-test_size:batch_size]\n",
    "print(test_categorical.shape)\n",
    "train_cont=cont_values[:batch_size-test_size]\n",
    "print(train_cont.shape)\n",
    "test_cont=cont_values[batch_size-test_size:batch_size]\n",
    "print(test_cont.shape)\n",
    "y_train=y[:batch_size-test_size]\n",
    "print(y_train.shape)\n",
    "y_test=y[batch_size-test_size:batch_size]\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 1 and loss: 200496.78125\n",
      "Epoch Number: 11 and loss: 200493.4375\n",
      "Epoch Number: 21 and loss: 200489.15625\n",
      "Epoch Number: 31 and loss: 200482.609375\n",
      "Epoch Number: 41 and loss: 200473.265625\n",
      "Epoch Number: 51 and loss: 200461.359375\n",
      "Epoch Number: 61 and loss: 200446.390625\n",
      "Epoch Number: 71 and loss: 200429.359375\n",
      "Epoch Number: 81 and loss: 200408.0\n",
      "Epoch Number: 91 and loss: 200383.421875\n",
      "Epoch Number: 101 and loss: 200355.3125\n",
      "Epoch Number: 111 and loss: 200322.125\n",
      "Epoch Number: 121 and loss: 200291.40625\n",
      "Epoch Number: 131 and loss: 200252.0\n",
      "Epoch Number: 141 and loss: 200206.609375\n",
      "Epoch Number: 151 and loss: 200162.234375\n",
      "Epoch Number: 161 and loss: 200112.234375\n",
      "Epoch Number: 171 and loss: 200059.65625\n",
      "Epoch Number: 181 and loss: 200005.875\n",
      "Epoch Number: 191 and loss: 199946.40625\n",
      "Epoch Number: 201 and loss: 199881.59375\n",
      "Epoch Number: 211 and loss: 199816.09375\n",
      "Epoch Number: 221 and loss: 199737.296875\n",
      "Epoch Number: 231 and loss: 199670.84375\n",
      "Epoch Number: 241 and loss: 199589.34375\n",
      "Epoch Number: 251 and loss: 199507.84375\n",
      "Epoch Number: 261 and loss: 199411.109375\n",
      "Epoch Number: 271 and loss: 199324.609375\n",
      "Epoch Number: 281 and loss: 199245.890625\n",
      "Epoch Number: 291 and loss: 199140.796875\n",
      "Epoch Number: 301 and loss: 199029.890625\n",
      "Epoch Number: 311 and loss: 198935.015625\n",
      "Epoch Number: 321 and loss: 198847.703125\n",
      "Epoch Number: 331 and loss: 198696.453125\n",
      "Epoch Number: 341 and loss: 198604.015625\n",
      "Epoch Number: 351 and loss: 198490.84375\n",
      "Epoch Number: 361 and loss: 198389.984375\n",
      "Epoch Number: 371 and loss: 198248.765625\n",
      "Epoch Number: 381 and loss: 198104.734375\n",
      "Epoch Number: 391 and loss: 198018.671875\n",
      "Epoch Number: 401 and loss: 197893.453125\n",
      "Epoch Number: 411 and loss: 197726.015625\n",
      "Epoch Number: 421 and loss: 197596.953125\n",
      "Epoch Number: 431 and loss: 197423.53125\n",
      "Epoch Number: 441 and loss: 197285.734375\n",
      "Epoch Number: 451 and loss: 197172.609375\n",
      "Epoch Number: 461 and loss: 196966.28125\n",
      "Epoch Number: 471 and loss: 196884.03125\n",
      "Epoch Number: 481 and loss: 196720.4375\n",
      "Epoch Number: 491 and loss: 196509.40625\n",
      "Epoch Number: 501 and loss: 196434.84375\n",
      "Epoch Number: 511 and loss: 196213.546875\n",
      "Epoch Number: 521 and loss: 196044.265625\n",
      "Epoch Number: 531 and loss: 195848.875\n",
      "Epoch Number: 541 and loss: 195663.78125\n",
      "Epoch Number: 551 and loss: 195456.78125\n",
      "Epoch Number: 561 and loss: 195288.84375\n",
      "Epoch Number: 571 and loss: 195069.65625\n",
      "Epoch Number: 581 and loss: 194842.75\n",
      "Epoch Number: 591 and loss: 194693.96875\n",
      "Epoch Number: 601 and loss: 194517.796875\n",
      "Epoch Number: 611 and loss: 194320.3125\n",
      "Epoch Number: 621 and loss: 194116.296875\n",
      "Epoch Number: 631 and loss: 193830.046875\n",
      "Epoch Number: 641 and loss: 193690.015625\n",
      "Epoch Number: 651 and loss: 193488.5625\n",
      "Epoch Number: 661 and loss: 193221.4375\n",
      "Epoch Number: 671 and loss: 193064.21875\n",
      "Epoch Number: 681 and loss: 192780.5\n",
      "Epoch Number: 691 and loss: 192498.578125\n",
      "Epoch Number: 701 and loss: 192299.421875\n",
      "Epoch Number: 711 and loss: 192216.078125\n",
      "Epoch Number: 721 and loss: 192017.578125\n",
      "Epoch Number: 731 and loss: 191619.84375\n",
      "Epoch Number: 741 and loss: 191425.421875\n",
      "Epoch Number: 751 and loss: 191196.75\n",
      "Epoch Number: 761 and loss: 190984.25\n",
      "Epoch Number: 771 and loss: 190729.640625\n",
      "Epoch Number: 781 and loss: 190469.40625\n",
      "Epoch Number: 791 and loss: 190289.890625\n",
      "Epoch Number: 801 and loss: 189873.40625\n",
      "Epoch Number: 811 and loss: 189755.171875\n",
      "Epoch Number: 821 and loss: 189660.03125\n",
      "Epoch Number: 831 and loss: 189295.328125\n",
      "Epoch Number: 841 and loss: 188947.453125\n",
      "Epoch Number: 851 and loss: 188835.515625\n",
      "Epoch Number: 861 and loss: 188393.171875\n",
      "Epoch Number: 871 and loss: 188182.390625\n",
      "Epoch Number: 881 and loss: 187898.859375\n",
      "Epoch Number: 891 and loss: 187495.65625\n",
      "Epoch Number: 901 and loss: 187445.265625\n",
      "Epoch Number: 911 and loss: 187225.328125\n",
      "Epoch Number: 921 and loss: 186795.953125\n",
      "Epoch Number: 931 and loss: 186372.203125\n",
      "Epoch Number: 941 and loss: 186139.21875\n",
      "Epoch Number: 951 and loss: 185956.796875\n",
      "Epoch Number: 961 and loss: 185622.375\n",
      "Epoch Number: 971 and loss: 185307.484375\n",
      "Epoch Number: 981 and loss: 185078.578125\n",
      "Epoch Number: 991 and loss: 184790.40625\n",
      "Epoch Number: 1001 and loss: 184317.046875\n",
      "Epoch Number: 1011 and loss: 184018.84375\n",
      "Epoch Number: 1021 and loss: 183813.65625\n",
      "Epoch Number: 1031 and loss: 183412.40625\n",
      "Epoch Number: 1041 and loss: 183223.703125\n",
      "Epoch Number: 1051 and loss: 182922.90625\n",
      "Epoch Number: 1061 and loss: 182569.46875\n",
      "Epoch Number: 1071 and loss: 182331.171875\n",
      "Epoch Number: 1081 and loss: 181899.3125\n",
      "Epoch Number: 1091 and loss: 181548.421875\n",
      "Epoch Number: 1101 and loss: 181324.5625\n",
      "Epoch Number: 1111 and loss: 180757.40625\n",
      "Epoch Number: 1121 and loss: 180487.21875\n",
      "Epoch Number: 1131 and loss: 180431.1875\n",
      "Epoch Number: 1141 and loss: 179934.84375\n",
      "Epoch Number: 1151 and loss: 179550.03125\n",
      "Epoch Number: 1161 and loss: 179362.5\n",
      "Epoch Number: 1171 and loss: 179042.90625\n",
      "Epoch Number: 1181 and loss: 178545.34375\n",
      "Epoch Number: 1191 and loss: 178112.1875\n",
      "Epoch Number: 1201 and loss: 178105.375\n",
      "Epoch Number: 1211 and loss: 177454.984375\n",
      "Epoch Number: 1221 and loss: 177493.1875\n",
      "Epoch Number: 1231 and loss: 176576.25\n",
      "Epoch Number: 1241 and loss: 176349.859375\n",
      "Epoch Number: 1251 and loss: 175528.8125\n",
      "Epoch Number: 1261 and loss: 175581.8125\n",
      "Epoch Number: 1271 and loss: 175242.796875\n",
      "Epoch Number: 1281 and loss: 174946.671875\n",
      "Epoch Number: 1291 and loss: 174483.578125\n",
      "Epoch Number: 1301 and loss: 174218.640625\n",
      "Epoch Number: 1311 and loss: 173677.328125\n",
      "Epoch Number: 1321 and loss: 173149.28125\n",
      "Epoch Number: 1331 and loss: 173396.90625\n",
      "Epoch Number: 1341 and loss: 172625.234375\n",
      "Epoch Number: 1351 and loss: 172267.265625\n",
      "Epoch Number: 1361 and loss: 172028.078125\n",
      "Epoch Number: 1371 and loss: 171849.453125\n",
      "Epoch Number: 1381 and loss: 171309.8125\n",
      "Epoch Number: 1391 and loss: 170379.140625\n",
      "Epoch Number: 1401 and loss: 170533.109375\n",
      "Epoch Number: 1411 and loss: 169669.734375\n",
      "Epoch Number: 1421 and loss: 169842.5625\n",
      "Epoch Number: 1431 and loss: 169494.28125\n",
      "Epoch Number: 1441 and loss: 168682.34375\n",
      "Epoch Number: 1451 and loss: 168647.9375\n",
      "Epoch Number: 1461 and loss: 167974.890625\n",
      "Epoch Number: 1471 and loss: 167604.328125\n",
      "Epoch Number: 1481 and loss: 167712.546875\n",
      "Epoch Number: 1491 and loss: 166595.875\n",
      "Epoch Number: 1501 and loss: 166381.40625\n",
      "Epoch Number: 1511 and loss: 165611.9375\n",
      "Epoch Number: 1521 and loss: 165897.71875\n",
      "Epoch Number: 1531 and loss: 165300.578125\n",
      "Epoch Number: 1541 and loss: 164557.96875\n",
      "Epoch Number: 1551 and loss: 164520.5625\n",
      "Epoch Number: 1561 and loss: 163728.515625\n",
      "Epoch Number: 1571 and loss: 163454.359375\n",
      "Epoch Number: 1581 and loss: 163062.5625\n",
      "Epoch Number: 1591 and loss: 162990.171875\n",
      "Epoch Number: 1601 and loss: 162405.546875\n",
      "Epoch Number: 1611 and loss: 161673.765625\n",
      "Epoch Number: 1621 and loss: 160812.703125\n",
      "Epoch Number: 1631 and loss: 160844.578125\n",
      "Epoch Number: 1641 and loss: 160780.0\n",
      "Epoch Number: 1651 and loss: 160150.921875\n",
      "Epoch Number: 1661 and loss: 160133.890625\n",
      "Epoch Number: 1671 and loss: 159053.359375\n",
      "Epoch Number: 1681 and loss: 158388.109375\n",
      "Epoch Number: 1691 and loss: 158326.546875\n",
      "Epoch Number: 1701 and loss: 157638.953125\n",
      "Epoch Number: 1711 and loss: 157295.453125\n",
      "Epoch Number: 1721 and loss: 157185.359375\n",
      "Epoch Number: 1731 and loss: 156653.5625\n",
      "Epoch Number: 1741 and loss: 156312.4375\n",
      "Epoch Number: 1751 and loss: 156190.234375\n",
      "Epoch Number: 1761 and loss: 154768.375\n",
      "Epoch Number: 1771 and loss: 154342.6875\n",
      "Epoch Number: 1781 and loss: 154102.796875\n",
      "Epoch Number: 1791 and loss: 153753.609375\n",
      "Epoch Number: 1801 and loss: 153283.09375\n",
      "Epoch Number: 1811 and loss: 152805.71875\n",
      "Epoch Number: 1821 and loss: 152282.53125\n",
      "Epoch Number: 1831 and loss: 152422.390625\n",
      "Epoch Number: 1841 and loss: 151603.890625\n",
      "Epoch Number: 1851 and loss: 150640.671875\n",
      "Epoch Number: 1861 and loss: 150838.703125\n",
      "Epoch Number: 1871 and loss: 150089.6875\n",
      "Epoch Number: 1881 and loss: 149300.53125\n",
      "Epoch Number: 1891 and loss: 148360.1875\n",
      "Epoch Number: 1901 and loss: 148627.21875\n",
      "Epoch Number: 1911 and loss: 148963.921875\n",
      "Epoch Number: 1921 and loss: 147738.71875\n",
      "Epoch Number: 1931 and loss: 147360.421875\n",
      "Epoch Number: 1941 and loss: 146662.671875\n",
      "Epoch Number: 1951 and loss: 146560.65625\n",
      "Epoch Number: 1961 and loss: 145976.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 1971 and loss: 145539.796875\n",
      "Epoch Number: 1981 and loss: 145761.125\n",
      "Epoch Number: 1991 and loss: 144515.828125\n",
      "Epoch Number: 2001 and loss: 144064.3125\n",
      "Epoch Number: 2011 and loss: 143061.359375\n",
      "Epoch Number: 2021 and loss: 143025.015625\n",
      "Epoch Number: 2031 and loss: 142482.453125\n",
      "Epoch Number: 2041 and loss: 141994.890625\n",
      "Epoch Number: 2051 and loss: 141090.546875\n",
      "Epoch Number: 2061 and loss: 140959.65625\n",
      "Epoch Number: 2071 and loss: 140982.640625\n",
      "Epoch Number: 2081 and loss: 139797.5\n",
      "Epoch Number: 2091 and loss: 139725.578125\n",
      "Epoch Number: 2101 and loss: 139362.265625\n",
      "Epoch Number: 2111 and loss: 138774.84375\n",
      "Epoch Number: 2121 and loss: 137884.671875\n",
      "Epoch Number: 2131 and loss: 137916.4375\n",
      "Epoch Number: 2141 and loss: 137216.75\n",
      "Epoch Number: 2151 and loss: 136502.421875\n",
      "Epoch Number: 2161 and loss: 136458.765625\n",
      "Epoch Number: 2171 and loss: 136542.4375\n",
      "Epoch Number: 2181 and loss: 135927.6875\n",
      "Epoch Number: 2191 and loss: 134856.671875\n",
      "Epoch Number: 2201 and loss: 134154.140625\n",
      "Epoch Number: 2211 and loss: 133836.734375\n",
      "Epoch Number: 2221 and loss: 132826.625\n",
      "Epoch Number: 2231 and loss: 132634.46875\n",
      "Epoch Number: 2241 and loss: 131628.703125\n",
      "Epoch Number: 2251 and loss: 131650.265625\n",
      "Epoch Number: 2261 and loss: 130343.1796875\n",
      "Epoch Number: 2271 and loss: 130352.796875\n",
      "Epoch Number: 2281 and loss: 130195.2421875\n",
      "Epoch Number: 2291 and loss: 129443.84375\n",
      "Epoch Number: 2301 and loss: 128923.7421875\n",
      "Epoch Number: 2311 and loss: 129019.1171875\n",
      "Epoch Number: 2321 and loss: 128285.0859375\n",
      "Epoch Number: 2331 and loss: 128558.125\n",
      "Epoch Number: 2341 and loss: 127630.0703125\n",
      "Epoch Number: 2351 and loss: 126300.953125\n",
      "Epoch Number: 2361 and loss: 126270.15625\n",
      "Epoch Number: 2371 and loss: 125693.7734375\n",
      "Epoch Number: 2381 and loss: 124634.703125\n",
      "Epoch Number: 2391 and loss: 124806.5078125\n",
      "Epoch Number: 2401 and loss: 124293.6796875\n",
      "Epoch Number: 2411 and loss: 124076.7890625\n",
      "Epoch Number: 2421 and loss: 124304.109375\n",
      "Epoch Number: 2431 and loss: 122356.515625\n",
      "Epoch Number: 2441 and loss: 122405.8671875\n",
      "Epoch Number: 2451 and loss: 121401.4453125\n",
      "Epoch Number: 2461 and loss: 120364.3828125\n",
      "Epoch Number: 2471 and loss: 120809.875\n",
      "Epoch Number: 2481 and loss: 120499.78125\n",
      "Epoch Number: 2491 and loss: 120638.328125\n",
      "Epoch Number: 2501 and loss: 119600.5703125\n",
      "Epoch Number: 2511 and loss: 118622.3359375\n",
      "Epoch Number: 2521 and loss: 117733.515625\n",
      "Epoch Number: 2531 and loss: 118230.9453125\n",
      "Epoch Number: 2541 and loss: 117380.9609375\n",
      "Epoch Number: 2551 and loss: 117051.34375\n",
      "Epoch Number: 2561 and loss: 116309.546875\n",
      "Epoch Number: 2571 and loss: 115739.6328125\n",
      "Epoch Number: 2581 and loss: 115934.8046875\n",
      "Epoch Number: 2591 and loss: 115576.1171875\n",
      "Epoch Number: 2601 and loss: 114420.25\n",
      "Epoch Number: 2611 and loss: 114364.96875\n",
      "Epoch Number: 2621 and loss: 114613.3671875\n",
      "Epoch Number: 2631 and loss: 112598.03125\n",
      "Epoch Number: 2641 and loss: 112124.3984375\n",
      "Epoch Number: 2651 and loss: 112175.265625\n",
      "Epoch Number: 2661 and loss: 111479.921875\n",
      "Epoch Number: 2671 and loss: 110855.0234375\n",
      "Epoch Number: 2681 and loss: 110172.6484375\n",
      "Epoch Number: 2691 and loss: 110075.6640625\n",
      "Epoch Number: 2701 and loss: 109775.109375\n",
      "Epoch Number: 2711 and loss: 109192.03125\n",
      "Epoch Number: 2721 and loss: 108944.15625\n",
      "Epoch Number: 2731 and loss: 108397.0\n",
      "Epoch Number: 2741 and loss: 107301.140625\n",
      "Epoch Number: 2751 and loss: 105801.8515625\n",
      "Epoch Number: 2761 and loss: 106179.7890625\n",
      "Epoch Number: 2771 and loss: 106602.1484375\n",
      "Epoch Number: 2781 and loss: 105264.1171875\n",
      "Epoch Number: 2791 and loss: 104547.1171875\n",
      "Epoch Number: 2801 and loss: 103416.6015625\n",
      "Epoch Number: 2811 and loss: 103988.0625\n",
      "Epoch Number: 2821 and loss: 102824.640625\n",
      "Epoch Number: 2831 and loss: 102763.984375\n",
      "Epoch Number: 2841 and loss: 102497.375\n",
      "Epoch Number: 2851 and loss: 101342.03125\n",
      "Epoch Number: 2861 and loss: 102089.6171875\n",
      "Epoch Number: 2871 and loss: 100638.0546875\n",
      "Epoch Number: 2881 and loss: 100899.5546875\n",
      "Epoch Number: 2891 and loss: 100509.125\n",
      "Epoch Number: 2901 and loss: 98651.6796875\n",
      "Epoch Number: 2911 and loss: 97953.0703125\n",
      "Epoch Number: 2921 and loss: 98098.40625\n",
      "Epoch Number: 2931 and loss: 98878.1328125\n",
      "Epoch Number: 2941 and loss: 96897.578125\n",
      "Epoch Number: 2951 and loss: 97338.390625\n",
      "Epoch Number: 2961 and loss: 96500.609375\n",
      "Epoch Number: 2971 and loss: 95403.5\n",
      "Epoch Number: 2981 and loss: 95270.8359375\n",
      "Epoch Number: 2991 and loss: 94389.2109375\n",
      "Epoch Number: 3001 and loss: 93715.0078125\n",
      "Epoch Number: 3011 and loss: 94202.8515625\n",
      "Epoch Number: 3021 and loss: 93325.4453125\n",
      "Epoch Number: 3031 and loss: 92691.0859375\n",
      "Epoch Number: 3041 and loss: 92890.5234375\n",
      "Epoch Number: 3051 and loss: 91447.34375\n",
      "Epoch Number: 3061 and loss: 91858.8046875\n",
      "Epoch Number: 3071 and loss: 91316.59375\n",
      "Epoch Number: 3081 and loss: 90886.765625\n",
      "Epoch Number: 3091 and loss: 89771.1953125\n",
      "Epoch Number: 3101 and loss: 89362.3671875\n",
      "Epoch Number: 3111 and loss: 89272.6015625\n",
      "Epoch Number: 3121 and loss: 89359.6953125\n",
      "Epoch Number: 3131 and loss: 87748.90625\n",
      "Epoch Number: 3141 and loss: 87642.046875\n",
      "Epoch Number: 3151 and loss: 87675.703125\n",
      "Epoch Number: 3161 and loss: 86143.453125\n",
      "Epoch Number: 3171 and loss: 87029.0234375\n",
      "Epoch Number: 3181 and loss: 86019.5859375\n",
      "Epoch Number: 3191 and loss: 85172.625\n",
      "Epoch Number: 3201 and loss: 84710.9296875\n",
      "Epoch Number: 3211 and loss: 84272.0078125\n",
      "Epoch Number: 3221 and loss: 83069.171875\n",
      "Epoch Number: 3231 and loss: 84091.171875\n",
      "Epoch Number: 3241 and loss: 82178.3828125\n",
      "Epoch Number: 3251 and loss: 81617.8125\n",
      "Epoch Number: 3261 and loss: 81357.1875\n",
      "Epoch Number: 3271 and loss: 81869.703125\n",
      "Epoch Number: 3281 and loss: 80971.609375\n",
      "Epoch Number: 3291 and loss: 82415.0546875\n",
      "Epoch Number: 3301 and loss: 79703.96875\n",
      "Epoch Number: 3311 and loss: 79169.140625\n",
      "Epoch Number: 3321 and loss: 79484.84375\n",
      "Epoch Number: 3331 and loss: 78477.4765625\n",
      "Epoch Number: 3341 and loss: 78260.4375\n",
      "Epoch Number: 3351 and loss: 77231.96875\n",
      "Epoch Number: 3361 and loss: 77063.53125\n",
      "Epoch Number: 3371 and loss: 76588.9296875\n",
      "Epoch Number: 3381 and loss: 76063.703125\n",
      "Epoch Number: 3391 and loss: 74903.203125\n",
      "Epoch Number: 3401 and loss: 74486.59375\n",
      "Epoch Number: 3411 and loss: 74287.71875\n",
      "Epoch Number: 3421 and loss: 73571.15625\n",
      "Epoch Number: 3431 and loss: 74002.8515625\n",
      "Epoch Number: 3441 and loss: 72505.015625\n",
      "Epoch Number: 3451 and loss: 72789.65625\n",
      "Epoch Number: 3461 and loss: 72584.4453125\n",
      "Epoch Number: 3471 and loss: 70682.53125\n",
      "Epoch Number: 3481 and loss: 71153.5859375\n",
      "Epoch Number: 3491 and loss: 70871.8125\n",
      "Epoch Number: 3501 and loss: 68740.453125\n",
      "Epoch Number: 3511 and loss: 69165.8515625\n",
      "Epoch Number: 3521 and loss: 69117.5234375\n",
      "Epoch Number: 3531 and loss: 68481.7421875\n",
      "Epoch Number: 3541 and loss: 68108.6484375\n",
      "Epoch Number: 3551 and loss: 67868.09375\n",
      "Epoch Number: 3561 and loss: 66179.0078125\n",
      "Epoch Number: 3571 and loss: 66365.3359375\n",
      "Epoch Number: 3581 and loss: 65905.1953125\n",
      "Epoch Number: 3591 and loss: 66266.359375\n",
      "Epoch Number: 3601 and loss: 64796.3046875\n",
      "Epoch Number: 3611 and loss: 65368.3359375\n",
      "Epoch Number: 3621 and loss: 65781.7109375\n",
      "Epoch Number: 3631 and loss: 63114.109375\n",
      "Epoch Number: 3641 and loss: 64588.34765625\n",
      "Epoch Number: 3651 and loss: 62384.953125\n",
      "Epoch Number: 3661 and loss: 64257.984375\n",
      "Epoch Number: 3671 and loss: 61820.0859375\n",
      "Epoch Number: 3681 and loss: 65081.49609375\n",
      "Epoch Number: 3691 and loss: 61781.92578125\n",
      "Epoch Number: 3701 and loss: 60169.25390625\n",
      "Epoch Number: 3711 and loss: 62347.296875\n",
      "Epoch Number: 3721 and loss: 60088.18359375\n",
      "Epoch Number: 3731 and loss: 57531.234375\n",
      "Epoch Number: 3741 and loss: 58184.671875\n",
      "Epoch Number: 3751 and loss: 58784.3671875\n",
      "Epoch Number: 3761 and loss: 58874.890625\n",
      "Epoch Number: 3771 and loss: 57346.2421875\n",
      "Epoch Number: 3781 and loss: 56130.3671875\n",
      "Epoch Number: 3791 and loss: 57590.8046875\n",
      "Epoch Number: 3801 and loss: 55665.9375\n",
      "Epoch Number: 3811 and loss: 55295.234375\n",
      "Epoch Number: 3821 and loss: 56353.7265625\n",
      "Epoch Number: 3831 and loss: 54163.109375\n",
      "Epoch Number: 3841 and loss: 53284.5703125\n",
      "Epoch Number: 3851 and loss: 55021.5078125\n",
      "Epoch Number: 3861 and loss: 54144.1640625\n",
      "Epoch Number: 3871 and loss: 52684.05859375\n",
      "Epoch Number: 3881 and loss: 51361.7890625\n",
      "Epoch Number: 3891 and loss: 50647.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 3901 and loss: 52845.61328125\n",
      "Epoch Number: 3911 and loss: 52261.05078125\n",
      "Epoch Number: 3921 and loss: 51437.01171875\n",
      "Epoch Number: 3931 and loss: 50745.5625\n",
      "Epoch Number: 3941 and loss: 49795.61328125\n",
      "Epoch Number: 3951 and loss: 49086.32421875\n",
      "Epoch Number: 3961 and loss: 50254.50390625\n",
      "Epoch Number: 3971 and loss: 49011.69921875\n",
      "Epoch Number: 3981 and loss: 49229.546875\n",
      "Epoch Number: 3991 and loss: 49491.36328125\n",
      "Epoch Number: 4001 and loss: 48217.4921875\n",
      "Epoch Number: 4011 and loss: 46850.67578125\n",
      "Epoch Number: 4021 and loss: 49075.05859375\n",
      "Epoch Number: 4031 and loss: 45564.66796875\n",
      "Epoch Number: 4041 and loss: 47119.859375\n",
      "Epoch Number: 4051 and loss: 47807.3671875\n",
      "Epoch Number: 4061 and loss: 44674.84375\n",
      "Epoch Number: 4071 and loss: 45843.1171875\n",
      "Epoch Number: 4081 and loss: 44161.8828125\n",
      "Epoch Number: 4091 and loss: 45711.6484375\n",
      "Epoch Number: 4101 and loss: 44162.375\n",
      "Epoch Number: 4111 and loss: 44180.94921875\n",
      "Epoch Number: 4121 and loss: 44138.5546875\n",
      "Epoch Number: 4131 and loss: 43434.8828125\n",
      "Epoch Number: 4141 and loss: 44674.7109375\n",
      "Epoch Number: 4151 and loss: 43302.28515625\n",
      "Epoch Number: 4161 and loss: 43029.99609375\n",
      "Epoch Number: 4171 and loss: 42299.5703125\n",
      "Epoch Number: 4181 and loss: 42607.26171875\n",
      "Epoch Number: 4191 and loss: 41643.078125\n",
      "Epoch Number: 4201 and loss: 42485.828125\n",
      "Epoch Number: 4211 and loss: 41984.1875\n",
      "Epoch Number: 4221 and loss: 39630.203125\n",
      "Epoch Number: 4231 and loss: 41879.1171875\n",
      "Epoch Number: 4241 and loss: 40591.08984375\n",
      "Epoch Number: 4251 and loss: 40036.078125\n",
      "Epoch Number: 4261 and loss: 40328.94921875\n",
      "Epoch Number: 4271 and loss: 40506.67578125\n",
      "Epoch Number: 4281 and loss: 42410.3671875\n",
      "Epoch Number: 4291 and loss: 39212.90234375\n",
      "Epoch Number: 4301 and loss: 39765.11328125\n",
      "Epoch Number: 4311 and loss: 39897.33984375\n",
      "Epoch Number: 4321 and loss: 39734.421875\n",
      "Epoch Number: 4331 and loss: 39433.42578125\n",
      "Epoch Number: 4341 and loss: 38179.59375\n",
      "Epoch Number: 4351 and loss: 41640.28515625\n",
      "Epoch Number: 4361 and loss: 39403.33203125\n",
      "Epoch Number: 4371 and loss: 38907.6484375\n",
      "Epoch Number: 4381 and loss: 37744.703125\n",
      "Epoch Number: 4391 and loss: 36775.59375\n",
      "Epoch Number: 4401 and loss: 37118.52734375\n",
      "Epoch Number: 4411 and loss: 38788.8828125\n",
      "Epoch Number: 4421 and loss: 39206.6328125\n",
      "Epoch Number: 4431 and loss: 37984.6015625\n",
      "Epoch Number: 4441 and loss: 38120.484375\n",
      "Epoch Number: 4451 and loss: 38524.88671875\n",
      "Epoch Number: 4461 and loss: 37760.0546875\n",
      "Epoch Number: 4471 and loss: 37922.38671875\n",
      "Epoch Number: 4481 and loss: 36531.17578125\n",
      "Epoch Number: 4491 and loss: 37425.03125\n",
      "Epoch Number: 4501 and loss: 37369.67578125\n",
      "Epoch Number: 4511 and loss: 36624.828125\n",
      "Epoch Number: 4521 and loss: 36474.1796875\n",
      "Epoch Number: 4531 and loss: 36711.1328125\n",
      "Epoch Number: 4541 and loss: 34773.0703125\n",
      "Epoch Number: 4551 and loss: 35662.98046875\n",
      "Epoch Number: 4561 and loss: 36809.015625\n",
      "Epoch Number: 4571 and loss: 37017.078125\n",
      "Epoch Number: 4581 and loss: 35388.51953125\n",
      "Epoch Number: 4591 and loss: 34876.5234375\n",
      "Epoch Number: 4601 and loss: 36133.50390625\n",
      "Epoch Number: 4611 and loss: 35761.60546875\n",
      "Epoch Number: 4621 and loss: 34646.90234375\n",
      "Epoch Number: 4631 and loss: 36004.46484375\n",
      "Epoch Number: 4641 and loss: 34538.7265625\n",
      "Epoch Number: 4651 and loss: 35855.58984375\n",
      "Epoch Number: 4661 and loss: 34181.609375\n",
      "Epoch Number: 4671 and loss: 36341.07421875\n",
      "Epoch Number: 4681 and loss: 35386.45703125\n",
      "Epoch Number: 4691 and loss: 38676.6328125\n",
      "Epoch Number: 4701 and loss: 34824.90234375\n",
      "Epoch Number: 4711 and loss: 34065.80859375\n",
      "Epoch Number: 4721 and loss: 35445.34375\n",
      "Epoch Number: 4731 and loss: 35879.8515625\n",
      "Epoch Number: 4741 and loss: 34838.23046875\n",
      "Epoch Number: 4751 and loss: 36915.34765625\n",
      "Epoch Number: 4761 and loss: 35022.68359375\n",
      "Epoch Number: 4771 and loss: 35724.58984375\n",
      "Epoch Number: 4781 and loss: 33603.78515625\n",
      "Epoch Number: 4791 and loss: 35168.2421875\n",
      "Epoch Number: 4801 and loss: 34578.78515625\n",
      "Epoch Number: 4811 and loss: 35675.31640625\n",
      "Epoch Number: 4821 and loss: 35500.30078125\n",
      "Epoch Number: 4831 and loss: 34955.14453125\n",
      "Epoch Number: 4841 and loss: 34782.640625\n",
      "Epoch Number: 4851 and loss: 36400.9375\n",
      "Epoch Number: 4861 and loss: 34823.57421875\n",
      "Epoch Number: 4871 and loss: 34651.69140625\n",
      "Epoch Number: 4881 and loss: 36988.08984375\n",
      "Epoch Number: 4891 and loss: 35456.85546875\n",
      "Epoch Number: 4901 and loss: 35457.37109375\n",
      "Epoch Number: 4911 and loss: 34109.125\n",
      "Epoch Number: 4921 and loss: 34617.17578125\n",
      "Epoch Number: 4931 and loss: 35817.66796875\n",
      "Epoch Number: 4941 and loss: 36405.8046875\n",
      "Epoch Number: 4951 and loss: 34131.24609375\n",
      "Epoch Number: 4961 and loss: 34519.375\n",
      "Epoch Number: 4971 and loss: 35883.7421875\n",
      "Epoch Number: 4981 and loss: 35455.96875\n",
      "Epoch Number: 4991 and loss: 32694.431640625\n",
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 5000\n",
    "final_losses = [] ## to store all loss value\n",
    "for i in range(epochs):\n",
    "    i=i+1\n",
    "    y_pred= model(train_categorical,train_cont) ## prediction value\n",
    "    loss=torch.sqrt(loss_function(y_pred,y_train)) ## calculating loss between actual and predicted value and converting to RMSE\n",
    "    final_losses.append(loss) ## appending all loss value to the list\n",
    "    if i%10==1:\n",
    "        print(\"Epoch Number: {} and loss: {}\".format(i,loss.item())) ##every 10 epochs print the loss value\n",
    "        \n",
    "    optimizer.zero_grad()## this function will reduce the loss after every epoch\n",
    "    loss.backward()## backpropagating the loss function by derivative\n",
    "    optimizer.step()## for single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### plot the loss function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "## loss plot\n",
    "plt.plot(range(epochs),final_losses)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 57231.2265625\n"
     ]
    }
   ],
   "source": [
    "#### validate the test data\n",
    "y_pred=\"\"\n",
    "with torch.no_grad():\n",
    "    y_pred=model(test_categorical,test_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,y_test))\n",
    "print('RMSE: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[130000.],\n",
       "        [138887.],\n",
       "        [175500.],\n",
       "        [195000.],\n",
       "        [142500.],\n",
       "        [265900.],\n",
       "        [224900.],\n",
       "        [248328.],\n",
       "        [170000.],\n",
       "        [465000.],\n",
       "        [178000.],\n",
       "        [186500.],\n",
       "        [129500.],\n",
       "        [119000.],\n",
       "        [244000.],\n",
       "        [130000.],\n",
       "        [165400.],\n",
       "        [127500.],\n",
       "        [301500.],\n",
       "        [ 99900.],\n",
       "        [190000.],\n",
       "        [151000.],\n",
       "        [128900.],\n",
       "        [180500.],\n",
       "        [181000.],\n",
       "        [183900.],\n",
       "        [122000.],\n",
       "        [378500.],\n",
       "        [144000.],\n",
       "        [177000.],\n",
       "        [139000.],\n",
       "        [137000.],\n",
       "        [237000.],\n",
       "        [ 68400.],\n",
       "        [227000.],\n",
       "        [180000.],\n",
       "        [150500.],\n",
       "        [139000.],\n",
       "        [169000.],\n",
       "        [132500.],\n",
       "        [278000.],\n",
       "        [281000.],\n",
       "        [119500.],\n",
       "        [107500.],\n",
       "        [162900.],\n",
       "        [115000.],\n",
       "        [138500.],\n",
       "        [155000.],\n",
       "        [140000.],\n",
       "        [160000.],\n",
       "        [154000.],\n",
       "        [290000.],\n",
       "        [232000.],\n",
       "        [130000.],\n",
       "        [325000.],\n",
       "        [202500.],\n",
       "        [138000.],\n",
       "        [147000.],\n",
       "        [335000.],\n",
       "        [203000.],\n",
       "        [333168.],\n",
       "        [119000.],\n",
       "        [206900.],\n",
       "        [295493.],\n",
       "        [208900.],\n",
       "        [111000.],\n",
       "        [156500.],\n",
       "        [190000.],\n",
       "        [ 82500.],\n",
       "        [147000.],\n",
       "        [ 55000.],\n",
       "        [ 79000.],\n",
       "        [130500.],\n",
       "        [256000.],\n",
       "        [176500.],\n",
       "        [227000.],\n",
       "        [132500.],\n",
       "        [100000.],\n",
       "        [125500.],\n",
       "        [125000.],\n",
       "        [167900.],\n",
       "        [135000.],\n",
       "        [ 52500.],\n",
       "        [200000.],\n",
       "        [128500.],\n",
       "        [123000.],\n",
       "        [155000.],\n",
       "        [177000.],\n",
       "        [155835.],\n",
       "        [108500.],\n",
       "        [283463.],\n",
       "        [122000.],\n",
       "        [200000.],\n",
       "        [171000.],\n",
       "        [134900.],\n",
       "        [410000.],\n",
       "        [170000.],\n",
       "        [315000.],\n",
       "        [189000.],\n",
       "        [260000.],\n",
       "        [156932.],\n",
       "        [144152.],\n",
       "        [193000.],\n",
       "        [127000.],\n",
       "        [232000.],\n",
       "        [105000.],\n",
       "        [165500.],\n",
       "        [274300.],\n",
       "        [250000.],\n",
       "        [239000.],\n",
       "        [ 91000.],\n",
       "        [117000.],\n",
       "        [ 83000.],\n",
       "        [167500.],\n",
       "        [ 58500.],\n",
       "        [157000.],\n",
       "        [105000.],\n",
       "        [125500.],\n",
       "        [250000.],\n",
       "        [136000.],\n",
       "        [377500.],\n",
       "        [131000.],\n",
       "        [235000.],\n",
       "        [124000.],\n",
       "        [123000.],\n",
       "        [163000.],\n",
       "        [246578.],\n",
       "        [281213.],\n",
       "        [137500.],\n",
       "        [138000.],\n",
       "        [137450.],\n",
       "        [120000.],\n",
       "        [193000.],\n",
       "        [193879.],\n",
       "        [282922.],\n",
       "        [105000.],\n",
       "        [275000.],\n",
       "        [133000.],\n",
       "        [125500.],\n",
       "        [215000.],\n",
       "        [230000.],\n",
       "        [140000.],\n",
       "        [ 90000.],\n",
       "        [257000.],\n",
       "        [207000.],\n",
       "        [175900.],\n",
       "        [122500.],\n",
       "        [124000.],\n",
       "        [179900.],\n",
       "        [127500.],\n",
       "        [136500.],\n",
       "        [142000.],\n",
       "        [271000.],\n",
       "        [140000.],\n",
       "        [119000.],\n",
       "        [192140.],\n",
       "        [ 64500.],\n",
       "        [186500.],\n",
       "        [160000.],\n",
       "        [174000.],\n",
       "        [120500.],\n",
       "        [394617.],\n",
       "        [149700.],\n",
       "        [197000.],\n",
       "        [191000.],\n",
       "        [310000.],\n",
       "        [179600.],\n",
       "        [129000.],\n",
       "        [240000.],\n",
       "        [112000.],\n",
       "        [ 92000.],\n",
       "        [136000.],\n",
       "        [287090.],\n",
       "        [145000.],\n",
       "        [ 84500.],\n",
       "        [185000.],\n",
       "        [175000.],\n",
       "        [210000.],\n",
       "        [266500.],\n",
       "        [142125.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verify=pd.DataFrame(y_test.tolist(),columns=[\"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted=pd.DataFrame(y_pred.tolist(),columns=[\"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>142821.59375</td>\n",
       "      <td>-12821.59375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138887.0</td>\n",
       "      <td>215776.00000</td>\n",
       "      <td>-76889.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175500.0</td>\n",
       "      <td>145253.31250</td>\n",
       "      <td>30246.68750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>206202.25000</td>\n",
       "      <td>-11202.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142500.0</td>\n",
       "      <td>217923.75000</td>\n",
       "      <td>-75423.75000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test    Prediction   Difference\n",
       "0  130000.0  142821.59375 -12821.59375\n",
       "1  138887.0  215776.00000 -76889.00000\n",
       "2  175500.0  145253.31250  30246.68750\n",
       "3  195000.0  206202.25000 -11202.25000\n",
       "4  142500.0  217923.75000 -75423.75000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output=pd.concat([data_verify,data_predicted],axis=1)\n",
    "final_output['Difference']=final_output['Test']-final_output['Prediction']\n",
    "final_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the model with weights\n",
    "torch.save(model,'HousePrice.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the model's weights\n",
    "torch.save(model.state_dict(),'HouseWeights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the previous model structure without any weights\n",
    "embs_size= [(15, 8), (5, 3), (2, 1), (4, 2)]\n",
    "model1=FeedForwardNN(embs_size,5,1,[100,50],p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the weights into blank model\n",
    "model1.load_state_dict(torch.load('HouseWeights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model summary\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
